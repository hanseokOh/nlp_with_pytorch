{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency and Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:23.247793Z",
     "start_time": "2020-02-14T09:49:23.241807Z"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = '''지능 지수 라는 말 들 어 보 셨 을 겁니다 . 여러분 의 지성 을 일컫 는 말 이 죠 . 그런데 심리 지수 란 건 뭘까요 ? 사람 들 이 특정 한 식 으로 행동 하 는 이유 에 대해 여러분 은 얼마나 알 고 계시 나요 ? 또 타인 이나 심지어 여러분 의 행동 을 예측 하 는 일 은 얼마나 잘 하 시 나요 ? 또 , 심리학 에 대해 갖춘 지식 중 에서 어느 정도 나 잘못 된 것 일까요 ? 심리학 에 관한 열 가지 신화 를 통해 잘못 된 것 들 을 알아보 도록 하 죠 . 여러분 은 한 번 쯤 들 어 보 셨 을 법 한 것 은 자신 들 의 심리학 에 대해 고려 할 때 , 거의 항상 남자 는 화성 에서 왔 고 , 여자 는 금성 에서 온 것 같 다고 합니다 . 하지만 실제로 남자 와 여자 는 얼마나 다른 걸까요 ? 이 를 알아보 기 위해 , 일단 남녀 사이 에 확실 하 게 차이 나 는 것 을 살펴보 고 심리학 적 인 성별 간 의 차이점 을 동일 한 척도 상 에서 대비 해 보 도록 하 겠 습니다 . 남자 와 여자 간 에 실제로 차이 나 는 능력 중 하나 는 그 들 이 공 을 얼마나 멀리 던질 수 있 느냐 하 는 것 입니다 . 여기 남자 들 의 데 이타 를 보 시 면 , 정상 분포 곡선 이 라는 걸 볼 수 있 습니다 . 남자 들 소수 는 정말 멀리 던지 고 , 남자 들 소수 는 멀리 던지 지 못하 지만 , 남자 들 대부분 은 평균 적 인 거리 를 던졌 습니다 . 여자 들 도 역시 비슷 한 분포 상태 를 보입니다 만 사실 남녀 사이 엔 커다란 차이 가 있 습니다 . 사실 , 평균 수준 의 남자 라면 모든 여성 중 대략 98 % 보다 더 멀리 던질 수 있 거든요 . 이 와 동일 하 게 표준 화 된 척도 상 에서 심리학 에서 말 하 는 성별 간 의 차이 를 살펴 봅시다 . 심리학자 라는 여러분 에게 말 하 길 남자 들 의 공간 지각 능력 이 여자 들 보다 뛰어나 다고 할 겁니다 . 예 를 들 어 , 지도 읽 는 능력 같 은 건데 , 맞 는 말 입니다 . 하지만 그 차이 의 정도 를 살펴봅시다 . 아주 작 죠 . 두 선 이 너무 근접 해서 거의 겹칠 정도 입니다 .\n",
    "'''\n",
    "\n",
    "doc2 = '''\n",
    "최상 의 제시 유형 은 학습 자 에 좌우 되 는 것 이 아니 라 학습 해야 할 내용 에 따라 좌우 됩니다 . 예 를 들 어 여러분 이 운전 하 기 를 배울 때 실제로 몸 으로 체감 하 는 경험 없이 누군가 가 어떻게 할 지 이야기 하 는 것 을 듣 는 것 만 으로 배울 수 있 습니까 ? 연립 방정식 을 풀 어야 하 는데 종이 에 쓰 지 않 고 머리 속 에서 말 하 는 것 으로 풀 수 가 있 을까요 ? 또는 만일 여러분 이 체감 형식 의 학습 자 유형 이 라면 , 건축학 시험 을 해석 적 춤 을 이용 하 여 수정 할 수 있 을까요 ? 아니 죠 ! 배워야 할 내용 을 제시 된 유형 에 맞추 어야 합니다 , 당신 에게 맞추 는 게 아니 라요 . 여러분 들 상당수 가 \" A \" 급 의 우등 생 이 라는 걸 아 는데 , 조만간 중등 학력 인증 시험 ( GCSE ) 결과 를 받 게 되 시 겠 네요 . 그런데 , 만일 , 여러분 들 이 희망 했 던 성적 을 받 지 못하 게 된다 해도 여러분 들 의 학습 방식 을 탓 해서 는 안 되 는 겁니다 . 여러분 이 비난 할 수 있 는 한 가지 는 바로 유전자 입니다 . 이건 최근 에 런던 대학교 ( UCL ) 에서 수행 했 던 연구 결과 는 여러 학생 들 과 그 들 의 중등 학력 인증 시험 결과 사이 의 차이 중 58 % 는 유전 적 인 요인 으로 좁혀졌 습니다 . 매우 정밀 한 수치 처럼 들립니다 . 그러면 어떻게 알 수 있 을까요 ? 유전 적 요인 과 환경 적 요인 의 상대 적 기여 도 를 알 고 싶 을 때 우리 가 사용 할 수 있 는 방식 은 바로 쌍둥이 연구 입니다 . 일 란 성 쌍생아 의 경우 환경 적 요인 과 유전 적 요인 모두 를 100 % 똑같이 공유 하 게 되 지만 이란 성 쌍생아 의 경우 는 100 % 동일 한 환경 을 공유 하 지만 유전자 의 경우 여타 의 형제자매 들 처럼 50 % 만 공유 하 게 됩니다 . 따라서 일 란 성 쌍둥이 와 이란 성 쌍둥이 사이 의 인증 시험 결과 가 얼마나 비슷 한지 비교 해 보 고 여기 에 약간 의 수학 적 계산 을 더하 게 되 면 그 수행 능력 의 차이 중 어느 정도 가 환경 적 요인 의 탓 이 고 어느 정도 가 유전자 탓 인지 를 알 수 있 게 됩니다 .\n",
    "'''\n",
    "\n",
    "doc3 = '''\n",
    "그러나 이 이야기 는 세 가지 이유 로 인해 신화 입니다 . 첫째 , 가장 중요 한 건 실험실 가운 은 흰색 이 아니 라 회색 이 었 다 라는 점 이 죠 . 둘째 , 참 여자 들 은 실험 하 기 전 에 와 참여 자 들 이 걱정 을 표현 할 때 마다 상기 시키 는 말 을 들 었 는데 , 전기 충격 이 고통 스럽 기 는 하 지만 , 치명 적 이 지 는 않 으며 실제로 영구 적 인 손상 을 남기 는 일 은 없 을 거 라는 것 이 었 습니다 . 셋째 , 참 여자 들 은 단지 가운 을 입 은 사람 이 시켜 전기 충격 을 주지 는 않 았 죠 . 실험 이 끝나 고 그 들 의 인터뷰 를 했 을 때 모든 참여 자 들 은 강한 신념 을 밝혔 는데 , ' 학습 과 처벌 ' 연구 가 과학 적 으로 가치 있 는 목적 을 수행 했 기 때문 에 비록 동료 참여 자 들 에게 가해진 순간 적 인 불편 함 에 반해서 과학 을 위해서 오래 남 을 성과 를 얻 을 것 이 라고 말 이 죠 . 그러 다 보 니 제 가 이야기 를 한 지 벌써 12 분 이 되 었 습니다 . 여러분 들 중 에 는 아마 거기 앉 아서 제 이야기 를 들으시는 동안 저 의 말투 와 몸짓 을 분석 하 면서 제 가 말 하 는 어떤 것 을 인지 해야 할까 해결 하 려고 하 셨 을 겁니다 , 제 가 진실 을 이야기 하 는 지 , 또는 거짓말 을 하 고 있 는 것 인지 말 이 죠 . 만일 그러 셨 다면 , 아마 지금 쯤 완전히 실패 하 셨 을 겁니다 . 왜냐하면 우리 모두 가 사람 이 말 하 는 패턴 과 몸짓 으로 도 거짓말 여부 를 알아내 는 것 이 가능 하 다고 생각 하 지만 , 오랜 세월 수백 회 에 걸쳐 행해진 실제 심리 검사 의 결과 를 보 면 우리 들 모두 는 , 심지어 경찰관 이나 탐정 들 을 포함 해서 도 기본 적 으로 몸짓 과 언어 적 패턴 으로 거짓말 을 탐지 하 는 것 은 운 에 맞 길 수 밖 에 는 없 는 것 입니다 . 흥미 롭 게 도 한 가지 예외 가 있 는데요 : 실종 된 친척 을 찾 아 달 라고 호소 하 는 TV 홍보 입니다 .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:24.204238Z",
     "start_time": "2020-02-14T09:49:23.669697Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pd.set_option('max_rows', 20)\n",
    "\n",
    "def get_term_frequency(document,word_dict=None):\n",
    "    '''\n",
    "    개별 문서에 대해서 TF (Term Frequency)를 구하는 함수\n",
    "    '''\n",
    "    if word_dict is None:\n",
    "        word_dict ={}\n",
    "    words = document.split()\n",
    "    \n",
    "    for w in words:\n",
    "        word_dict[w] = 1+(0 if word_dict.get(w) is None else word_dict[w])\n",
    "    \n",
    "    return pd.Series(word_dict).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def get_document_frequency(documents):\n",
    "    '''\n",
    "    문서들이 주어졌을 때 각 단어가 몇 개의 문서에서 나타나는지 세는 함수\n",
    "    '''\n",
    "    dicts =[]\n",
    "    vocab = set([])\n",
    "    df= {}\n",
    "    \n",
    "    for d in documents:\n",
    "        tf = get_term_frequency(d)\n",
    "        dicts +=[tf]\n",
    "        vocab = vocab | set(tf.keys()) # list의 append와 같이 set 형태에 자료를 추가하는 형식\n",
    "        \n",
    "    for v in list(vocab):\n",
    "        df[v]=0\n",
    "        for dict_d in dicts:\n",
    "            if dict_d.get(v) is not None:\n",
    "                df[v]+=1 # document에 특정 단어 v가 존재한다.\n",
    "    \n",
    "    return pd.Series(df).sort_values(ascending=False)\n",
    "\n",
    "def get_tfidf(docs):\n",
    "    '''\n",
    "    TF-IDF를 구하는 최종 함수\n",
    "    '''\n",
    "    vocab = {}\n",
    "    tfs = []\n",
    "    for d in docs:\n",
    "        vocab = get_term_frequency(d,vocab)\n",
    "        tfs += [get_term_frequency(d)]\n",
    "    df = get_document_frequency(docs)\n",
    "    \n",
    "    from operator import itemgetter\n",
    "    import numpy as np\n",
    "    \n",
    "    stats = []\n",
    "    for word,freq in vocab.items():\n",
    "        tfidfs=[]\n",
    "        for idx in range(len(docs)): \n",
    "            if tfs[idx].get(word) is not None:\n",
    "                tfidfs += [tfs[idx][word] * np.log(len(docs)/ df[word])] # TF / IDF \n",
    "            else:\n",
    "                tfidfs+=[0]\n",
    "        \n",
    "        stats.append((word, freq, *tfidfs, max(tfidfs)))\n",
    "        \n",
    "    return pd.DataFrame(stats, columns=('word',\n",
    "                                       'frequency',\n",
    "                                       'doc1',\n",
    "                                       'doc2',\n",
    "                                       'doc3',\n",
    "                                       'max')).sort_values('max',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:24.558287Z",
     "start_time": "2020-02-14T09:49:24.513435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "는      3\n",
       "실제로    3\n",
       ".      3\n",
       "그      3\n",
       "고      3\n",
       "      ..\n",
       "열      1\n",
       "얻      1\n",
       "롭      1\n",
       "속      1\n",
       "척도     1\n",
       "Length: 437, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_document_frequency([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:27.173326Z",
     "start_time": "2020-02-14T09:49:26.918973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>남자</td>\n",
       "      <td>9</td>\n",
       "      <td>9.887511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.887511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>요인</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.591674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.591674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>심리학</td>\n",
       "      <td>5</td>\n",
       "      <td>5.493061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.493061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>었</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.394449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>제</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.394449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>라는</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>중</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>습니다</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>보</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>는</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  frequency      doc1      doc2      doc3       max\n",
       "23   남자          9  9.887511  0.000000  0.000000  9.887511\n",
       "37   요인          6  0.000000  6.591674  0.000000  6.591674\n",
       "51  심리학          5  5.493061  0.000000  0.000000  5.493061\n",
       "57    었          4  0.000000  0.000000  4.394449  4.394449\n",
       "63    제          4  0.000000  0.000000  4.394449  4.394449\n",
       "..  ...        ...       ...       ...       ...       ...\n",
       "36   라는          6  0.000000  0.000000  0.000000  0.000000\n",
       "33    중          6  0.000000  0.000000  0.000000  0.000000\n",
       "30  습니다          7  0.000000  0.000000  0.000000  0.000000\n",
       "29    보          7  0.000000  0.000000  0.000000  0.000000\n",
       "0     는         47  0.000000  0.000000  0.000000  0.000000\n",
       "\n",
       "[437 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:51.560171Z",
     "start_time": "2020-02-14T09:49:51.552193Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tf(docs):\n",
    "    vocab= {}\n",
    "    tfs = []\n",
    "    for d in docs:\n",
    "        vocab = get_term_frequency(d,vocab)\n",
    "        tfs +=[get_term_frequency(d)]\n",
    "        \n",
    "    from operator import itemgetter\n",
    "    import numpy as np\n",
    "    \n",
    "    stats =[]\n",
    "    for word, freq in vocab.items():\n",
    "        tf_v = []\n",
    "        for idx in range(len(docs)):\n",
    "            if tfs[idx].get(word) is not None:\n",
    "                tf_v +=[tfs[idx][word]]\n",
    "                \n",
    "            else:\n",
    "                tf_v +=[0]\n",
    "        stats.append((word,freq, *tf_v))\n",
    "        \n",
    "    return pd.DataFrame(stats,columns=('word',\n",
    "                                      'frequency',\n",
    "                                      'doc1',\n",
    "                                      'doc2',\n",
    "                                      'doc3')).sort_values('frequency',ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:52.833769Z",
     "start_time": "2020-02-14T09:49:52.592412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>는</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>을</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>하</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>이</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>스럽</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>치명</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>으며</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>영구</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>최근</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency  doc1  doc2  doc3\n",
       "0      는         47    15    14    18\n",
       "1      을         39     8    10    21\n",
       "2      .         36    16    10    10\n",
       "3      하         33    10     9    14\n",
       "4      이         32     8     8    16\n",
       "..   ...        ...   ...   ...   ...\n",
       "273   스럽          1     0     0     1\n",
       "274   치명          1     0     0     1\n",
       "275   으며          1     0     0     1\n",
       "276   영구          1     0     0     1\n",
       "436   최근          1     0     1     0\n",
       "\n",
       "[437 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Context Window (Co-occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:54.822441Z",
     "start_time": "2020-02-14T09:49:54.343701Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "해당 파일은 다음 주소에서 가져옴\n",
    "https://github.com/juneoh/fastcampus-pytorch-nlp/blob/master/week3/ted.aligned.ko.refined.tok.txt\n",
    "'''\n",
    "with open('ted.aligned.ko.refined.tok.txt',encoding='utf-8') as f:\n",
    "    lines = [l.strip() for l in f.read().splitlines() if l.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:49:55.418823Z",
     "start_time": "2020-02-14T09:49:55.412838Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_context_counts(lines,w_size=2):\n",
    "    '''\n",
    "    문장들을 입력으로 받아 주어진 윈도우 크기 w_size 내에서 함께 출현한 단어들의 빈도를 세는 함수\n",
    "    '''\n",
    "    co_dict = defaultdict(int)\n",
    "    \n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        \n",
    "        for i,w in enumerate(words):\n",
    "            for c in words[i - w_size:i + w_size]:\n",
    "                if w!=c:\n",
    "                    co_dict[(w,c)]+=1\n",
    "    \n",
    "    return pd.Series(co_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:50:15.568926Z",
     "start_time": "2020-02-14T09:49:56.323405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "웹    현재        1\n",
       "     TED       2\n",
       "     사이트     350\n",
       "사이트  TED       3\n",
       "     웹       365\n",
       "            ... \n",
       "산책   바퀴벌레      1\n",
       "말벌   시키        1\n",
       "대해   말벌        1\n",
       "기생충  안         1\n",
       "생각   기생충       1\n",
       "Length: 2706519, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_dict = get_context_counts(lines)\n",
    "co_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:50:22.117449Z",
     "start_time": "2020-02-14T09:50:19.455559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".      337685\n",
       "는      234692\n",
       "이      224422\n",
       "을      193583\n",
       "은      137351\n",
       "        ...  \n",
       "711         1\n",
       "967         1\n",
       "선거제         1\n",
       "아쉬웠         1\n",
       "끼친다         1\n",
       "Length: 62960, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = get_term_frequency(' '.join(lines))\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:50:26.641313Z",
     "start_time": "2020-02-14T09:50:26.603436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "고      98136\n",
       "것      78869\n",
       "한      63132\n",
       "죠      56568\n",
       "그      53895\n",
       "       ...  \n",
       "711        1\n",
       "967        1\n",
       "선거제        1\n",
       "아쉬웠        1\n",
       "끼친다        1\n",
       "Length: 62946, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = tfs[tfs<100000]\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:51:17.242964Z",
     "start_time": "2020-02-14T09:50:27.273620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        고     것     한    죠     그    입니다    우리     수    에서     었  ...  시기  신호  \\\n",
      "고       0  1034   165    7  1788      8   839  1288   924  3204  ...   6   8   \n",
      "것     364     0  5473   13   282  15019    36     4   563  1109  ...   5   3   \n",
      "한    1670  5202     0  104   529     57   267    54  1252    41  ...  75  27   \n",
      "죠    1307  2636   363    0   153      2   107  1391    96  5316  ...  20  12   \n",
      "그    2939   568   219  827     0    578  1045    46   628   517  ...  34  19   \n",
      "..    ...   ...   ...  ...   ...    ...   ...   ...   ...   ...  ...  ..  ..   \n",
      "딸      11     4     3    3     9     11    24     6     3     0  ...   0   0   \n",
      "신체     10     5    20    1     8      5    49     0    24     0  ...   0   1   \n",
      "미국인     8     1    22    3     2     13     8     1     8     3  ...   0   0   \n",
      "만일     10    12     0   10    26     16    53     0     2     3  ...   0   0   \n",
      "생명체     6     4    38    5     7      8     8     0    40     0  ...   0   0   \n",
      "\n",
      "      운영   다루  갑자기   딸  신체  미국인  만일  생명체  \n",
      "고    127  103   17  10   5   11   1    5  \n",
      "것      6   37   11   2   1    0   2    3  \n",
      "한      5    2   20   4  20   16   8   32  \n",
      "죠      7   12    1   0   0    2   3    3  \n",
      "그      1    0   28   6   5    5  28    7  \n",
      "..   ...  ...  ...  ..  ..  ...  ..  ...  \n",
      "딸      0    0    0   0   0    0   0    0  \n",
      "신체     0    0    0   0   0    0   0    0  \n",
      "미국인    0    0    0   0   0    0   2    0  \n",
      "만일     0    0    0   0   0    1   0    0  \n",
      "생명체    0    0    0   0   0    0   0    0  \n",
      "\n",
      "[1000 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "def co_occurence(co_dict,vocab):\n",
    "    data = []\n",
    "    \n",
    "    for word1 in vocab:\n",
    "        row = []\n",
    "        \n",
    "        for word2 in vocab:\n",
    "            try:\n",
    "                count = co_dict[(word1,word2)]\n",
    "            except KeyError:\n",
    "                count=0\n",
    "            row.append(count)\n",
    "        \n",
    "        data.append(row)\n",
    "        \n",
    "    return pd.DataFrame(data,index= vocab, columns = vocab)                \n",
    "\n",
    "co = co_occurence(co_dict, tfs.index[:1000])\n",
    "print(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:51:21.493626Z",
     "start_time": "2020-02-14T09:51:21.115633Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(co, 'co.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:51:22.060124Z",
     "start_time": "2020-02-14T09:51:22.016198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>고</th>\n",
       "      <th>것</th>\n",
       "      <th>한</th>\n",
       "      <th>죠</th>\n",
       "      <th>그</th>\n",
       "      <th>입니다</th>\n",
       "      <th>우리</th>\n",
       "      <th>수</th>\n",
       "      <th>에서</th>\n",
       "      <th>었</th>\n",
       "      <th>...</th>\n",
       "      <th>시기</th>\n",
       "      <th>신호</th>\n",
       "      <th>운영</th>\n",
       "      <th>다루</th>\n",
       "      <th>갑자기</th>\n",
       "      <th>딸</th>\n",
       "      <th>신체</th>\n",
       "      <th>미국인</th>\n",
       "      <th>만일</th>\n",
       "      <th>생명체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>고</td>\n",
       "      <td>0</td>\n",
       "      <td>1034</td>\n",
       "      <td>165</td>\n",
       "      <td>7</td>\n",
       "      <td>1788</td>\n",
       "      <td>8</td>\n",
       "      <td>839</td>\n",
       "      <td>1288</td>\n",
       "      <td>924</td>\n",
       "      <td>3204</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>127</td>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>것</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>5473</td>\n",
       "      <td>13</td>\n",
       "      <td>282</td>\n",
       "      <td>15019</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>563</td>\n",
       "      <td>1109</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>한</td>\n",
       "      <td>1670</td>\n",
       "      <td>5202</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>529</td>\n",
       "      <td>57</td>\n",
       "      <td>267</td>\n",
       "      <td>54</td>\n",
       "      <td>1252</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>죠</td>\n",
       "      <td>1307</td>\n",
       "      <td>2636</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>1391</td>\n",
       "      <td>96</td>\n",
       "      <td>5316</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>그</td>\n",
       "      <td>2939</td>\n",
       "      <td>568</td>\n",
       "      <td>219</td>\n",
       "      <td>827</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>1045</td>\n",
       "      <td>46</td>\n",
       "      <td>628</td>\n",
       "      <td>517</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>딸</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>신체</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>미국인</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>만일</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>생명체</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        고     것     한    죠     그    입니다    우리     수    에서     었  ...  시기  신호  \\\n",
       "고       0  1034   165    7  1788      8   839  1288   924  3204  ...   6   8   \n",
       "것     364     0  5473   13   282  15019    36     4   563  1109  ...   5   3   \n",
       "한    1670  5202     0  104   529     57   267    54  1252    41  ...  75  27   \n",
       "죠    1307  2636   363    0   153      2   107  1391    96  5316  ...  20  12   \n",
       "그    2939   568   219  827     0    578  1045    46   628   517  ...  34  19   \n",
       "..    ...   ...   ...  ...   ...    ...   ...   ...   ...   ...  ...  ..  ..   \n",
       "딸      11     4     3    3     9     11    24     6     3     0  ...   0   0   \n",
       "신체     10     5    20    1     8      5    49     0    24     0  ...   0   1   \n",
       "미국인     8     1    22    3     2     13     8     1     8     3  ...   0   0   \n",
       "만일     10    12     0   10    26     16    53     0     2     3  ...   0   0   \n",
       "생명체     6     4    38    5     7      8     8     0    40     0  ...   0   0   \n",
       "\n",
       "      운영   다루  갑자기   딸  신체  미국인  만일  생명체  \n",
       "고    127  103   17  10   5   11   1    5  \n",
       "것      6   37   11   2   1    0   2    3  \n",
       "한      5    2   20   4  20   16   8   32  \n",
       "죠      7   12    1   0   0    2   3    3  \n",
       "그      1    0   28   6   5    5  28    7  \n",
       "..   ...  ...  ...  ..  ..  ...  ..  ...  \n",
       "딸      0    0    0   0   0    0   0    0  \n",
       "신체     0    0    0   0   0    0   0    0  \n",
       "미국인    0    0    0   0   0    0   2    0  \n",
       "만일     0    0    0   0   0    1   0    0  \n",
       "생명체    0    0    0   0   0    0   0    0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = torch.load('co.pth')\n",
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:37:21.175584Z",
     "start_time": "2020-02-14T12:37:21.171625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'생명체' in co.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE (t-distributed stochastic neighbor embedding) 을 이용해서 고차원의 vector를 시각화 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:54:27.621312Z",
     "start_time": "2020-02-14T09:54:27.617321Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{d}_{\\text{L1}}(w,v)=\\sum_{i=1}^d{|w_i-v_i|},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:55:21.368712Z",
     "start_time": "2020-02-14T09:55:21.363710Z"
    }
   },
   "outputs": [],
   "source": [
    "# L1 distance\n",
    "def get_l1_distance(x1,x2):\n",
    "    return ((x1-x2)).abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{d}_{\\text{L2}}(w,v)=\\sqrt{\\sum_{i=1}^d{(w_i-v_i)^2}},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T10:01:24.788184Z",
     "start_time": "2020-02-14T10:01:24.784228Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2 distance\n",
    "def get_l2_distance(x1,x2):\n",
    "    return ((x1-x2)**2).sum()**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d_{\\infty}(w,v)=\\max(|w_1-v_1|,|w_2-v_2|,\\cdots,|w_d-v_d|),\\text{ where }w,v\\in\\mathbb{R}^d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T10:01:34.174081Z",
     "start_time": "2020-02-14T10:01:34.169094Z"
    }
   },
   "outputs": [],
   "source": [
    "# infinity Norm\n",
    "def get_infinity_distance(x1,x2):\n",
    "    return ((x1-x2).abs()).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}_{\\text{cos}}(w,v)&amp;=\\overbrace{\\frac{w\\cdot v}{|w||v|}}^{\\text{dot product}}\n",
    "=\\overbrace{\\frac{w}{|w|}}^{\\text{unit vector}}\\cdot\\frac{v}{|v|} \\\\\n",
    "&amp;=\\frac{\\sum_{i=1}^{d}{w_iv_i}}{\\sqrt{\\sum_{i=1}^d{w_i^2}}\\sqrt{\\sum_{i=1}^d{v_i^2}}} \\\\\n",
    "\\text{where }&amp;w,v\\in\\mathbb{R}^d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:36:03.567048Z",
     "start_time": "2020-02-14T12:36:03.563086Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(x1,x2):\n",
    "    return (x1*x2).sum() / ((x1**2).sum()**.5 *(x2**2).sum()**.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}_{\\text{jaccard}}(w,v)&amp;=\\frac{|w \\cap v|}{|w \\cup v|} \\\\\n",
    "&amp;=\\frac{|w \\cap v|}{|w|+|v|-|w \\cap v|} \\\\\n",
    "&amp;\\approx\\frac{\\sum_{i=1}^d{\\min(w_i,v_i)}}{\\sum_{i=1}^d{\\max(w_i,v_i)}} \\\\\n",
    "\\text{where }&amp;w,v\\in\\mathbb{R}^d.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T10:11:25.780452Z",
     "start_time": "2020-02-14T10:11:25.775488Z"
    }
   },
   "outputs": [],
   "source": [
    "# 자카드 유사도\n",
    "def get_jaccard_similarity(x1,x2):\n",
    "    return torch.stack([x1,x2]).min(dim=0)[0].sum() / torch.stack([x1,x2]).max(dim=0)[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값이 아니라 수치 자체에 대해 자카드 유사도를 구하고자 할 때는, 각 차원의 숫자에 위와 같이 min,max 연산을 통해서 계산 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:09:01.355571Z",
     "start_time": "2020-02-14T11:09:01.347599Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nearest(query, dataframe,metric,top_k,ascending=True):\n",
    "    vector = torch.FloatTensor(dataframe.loc[query].values)\n",
    "    distances = dataframe.apply(lambda x: metric(vector, torch.FloatTensor(x.values)),axis=1)\n",
    "    top_distances = distances.sort_values(ascending=ascending)[:top_k]\n",
    "    \n",
    "    print(', '.join([f'{k} ({v:.1f})' for k,v in top_distances.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:09:03.114831Z",
     "start_time": "2020-02-14T11:09:02.292035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 distance: \n",
      "우리 (0.0), 저 (24246.0), 제 (26486.0), 여러분 (30102.0), 그 (33590.0), 그것 (34206.0), 이런 (34273.0), 이것 (34569.0), 그리고 (34732.0), 어떤 (35048.0), 나 (35285.0), ' (35615.0), -- (35832.0), 요 (35935.0), 그런 (36570.0), 당신 (36757.0), 바로 (36803.0), 여기 (36804.0), 하지만 (36811.0), 그래서 (36900.0), 어떻게 (36977.0), 다 (37061.0), 저희 (37277.0), 모든 (37366.0), 살 (37483.0), 미국 (37631.0), 새로운 (37684.0), 다른 (37693.0), 사실 (37719.0), 까지 (37754.0)\n",
      "\n",
      "L2 distance: \n",
      "우리 (0.0), 저 (2180.6), 제 (2695.3), 여러분 (2839.4), 그 (3223.8), 나 (3225.2), 그것 (3231.0), 이런 (3295.4), 이것 (3322.2), 어떤 (3350.7), 살 (3404.7), 당신 (3460.9), 그리고 (3480.9), 자신 (3533.2), ' (3540.5), 아이 (3544.4), -- (3547.8), 다 (3548.8), 일어나 (3550.0), 저희 (3550.7), 그런 (3567.0), 그녀 (3570.0), 매우 (3571.6), 안 (3571.7), 어떻게 (3580.0), 이야기 (3598.4), 갖 (3599.2), 요 (3599.8), 여기 (3600.2), 아주 (3600.8)\n",
      "\n",
      "Infinity distance: \n",
      "우리 (0.0), 저 (852.0), 여러분 (1031.0), 자신 (1309.0), 모두 (1316.0), 나 (1376.0), 물 (1396.0), 당신 (1396.0), 영향 (1424.0), 스스로 (1434.0), 그녀 (1447.0), 필요 (1456.0), 어떤 (1459.0), 이런 (1465.0), 서로 (1481.0), 아이 (1483.0), 그 (1485.0), 이렇게 (1487.0), 만 (1487.0), 가장 (1499.0), 누구 (1501.0), 저희 (1501.0), 이야기 (1506.0), 로 (1506.0), 도움 (1511.0), 큰 (1512.0), 매우 (1517.0), 일 (1518.0), 질문 (1519.0), 보여 (1521.0)\n",
      "\n",
      "Cosine similarity: \n",
      "우리 (1.0), 저희 (0.9), 저 (0.8), 그녀 (0.8), 그 (0.8), 여러분 (0.7), 제 (0.7), 당신 (0.7), 누군가 (0.7), 그것 (0.7), 우린 (0.7), 새 (0.7), 일종 (0.7), 이것 (0.7), 환자 (0.7), 하루 (0.7), 그냥 (0.6), 마치 (0.6), 전화 (0.6), 각각 (0.6), 인터넷 (0.6), 컴퓨터 (0.6), 수많 (0.6), 뭔가 (0.6), 빛 (0.6), 누가 (0.6), 로봇 (0.6), 약간 (0.6), 물론 (0.6), 나무 (0.6)\n",
      "\n",
      "Jaccard similarity: \n",
      "우리 (1.0), 그 (0.5), 저 (0.5), 제 (0.4), 여러분 (0.3), 말 (0.3), \" (0.3), 그리고 (0.3), 사람 (0.3), 일 (0.3), 도 (0.2), 다른 (0.2), 나 (0.2), 이런 (0.2), 한 (0.2), 만 (0.2), 모든 (0.2), 어떤 (0.2), 더 (0.2), 와 (0.2), 에서 (0.2), 과 (0.2), 로 (0.2), 보 (0.2), 요 (0.2), 된 (0.2), ' (0.2), 그것 (0.2), 적 (0.2), 되 (0.2)\n"
     ]
    }
   ],
   "source": [
    "print('L1 distance: ')\n",
    "get_nearest('우리',co,get_l1_distance,30)\n",
    "print('\\nL2 distance: ')\n",
    "get_nearest('우리',co,get_l2_distance,30)\n",
    "print('\\nInfinity distance: ')\n",
    "get_nearest('우리',co,get_infinity_distance,30)\n",
    "print('\\nCosine similarity: ')\n",
    "get_nearest('우리',co,get_cosine_similarity,30,ascending=False)\n",
    "print('\\nJaccard similarity: ')\n",
    "get_nearest('우리',co,get_jaccard_similarity,30,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesaurus Based Method: Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:10:43.296872Z",
     "start_time": "2020-02-14T11:10:40.375724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:11:49.052198Z",
     "start_time": "2020-02-14T11:11:49.046215Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesk(sentence, word):\n",
    "    from nltk.wsd import lesk\n",
    "    \n",
    "    best_synset= lesk(sentence.split(),word)\n",
    "    print(best_synset, best_synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:12:37.956392Z",
     "start_time": "2020-02-14T11:12:37.948442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I went fishing last weekend and I got a bass and cooked it'\n",
    "word = 'bass'\n",
    "lesk(sentence,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:14:03.812735Z",
     "start_time": "2020-02-14T11:14:03.803787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.02') the lowest part in polyphonic music\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love the music from the speaker which has strong beat and bass'\n",
    "word = 'bass'\n",
    "lesk(sentence,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:15:03.518360Z",
     "start_time": "2020-02-14T11:15:03.510382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "# 잘 안된 케이스\n",
    "sentence = 'I think the bass is more important than guitar'\n",
    "word='bass'\n",
    "lesk(sentence,word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Based Method [Erk et al.2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MACOSX_DEPLOYMENT_TARGET=10.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpype\n",
    "jpype.getDefaultJVMPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:53:24.799762Z",
     "start_time": "2020-02-14T12:52:17.112719Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma # Mecab 설치시 고려해야할 것이 많기에 대체\n",
    "\n",
    "with open('ted.aligned.ko.refined.tok.random-10k.txt',encoding='utf-8') as f:\n",
    "    lines = [l.strip() for l in f.read().splitlines() if l.strip()]\n",
    "    \n",
    "def count_seen_headwords(lines, predicate = 'VV',headword='NNG'):\n",
    "    '''\n",
    "    params\n",
    "    * lines : 코퍼스\n",
    "    * predicate : 술어 , 'VV'-> 동사\n",
    "    * headword : 표제어, 'NNG' -> 명사\n",
    "    '''\n",
    "    tagger= Kkma()\n",
    "    seen_dict = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        pos_result = tagger.pos(line)\n",
    "        \n",
    "        word_h = None\n",
    "        word_p = None\n",
    "        for word,pos in pos_result:\n",
    "            if pos == predicate or pos[:3] ==predicate + '+':\n",
    "                word_p = word\n",
    "                break\n",
    "            if pos == headword:\n",
    "                word_h = word\n",
    "                \n",
    "        if word_h is not None and word_p is not None:\n",
    "            seen_dict[word_p] = [word_h] + ([] if seen_dict.get(word_p) is None else seen_dict[word_p])\n",
    "        \n",
    "    return seen_dict\n",
    "\n",
    "seen_headwords = count_seen_headwords(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:54:08.719326Z",
     "start_time": "2020-02-14T12:54:08.712399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['하', '오', '늘', '겪', '들', '만드', '저', '관하', '가', '벌어', '푸', '일으키', '일어나', '되', '슬', '쫓', '피하', '키', '찾', '치', '맺', '생기', '졀', '스', '걷', '시', '믿', '만들', '가르치', '보', '알', '쓰', '꾸', '고맙', '물러나', '따', '뀌', '노', '나가', '통하', '아니', '쓸', '그러', '나', '앞장서', '좋', '느끼', '어', '느', '살피', '홀리', '웨', '에', '떠다니', '대하', '받', '가지', '드리', '앉', '갈', '드', '밝히', '마', '닫히', '흘러내리', '밀리', '보이', '보내', '지치', '잃', '여기', '달', '빼앗기', '매', '바꾸', '움직이', '걸리', '어쩌', '걸', '주', '걸치', '얻', '그리', '지나', '베', '트', '딛', '의하', '파', '뜨', '인하', '돌아가', '없애', '벗', '자', '열리', '버리', '그리하', '이르', '가져오', '밝혀지', '말', '주어지', '넣', '티', '나타나', '뻗', '위하', '읽', '없', '쉬', '시키', '뚫리', '시켰', '놀', '밝', '뛰', '서', '그', '따라오', '맛보', '이러', '째', '사', '아', '바라보', '내려가', '해지', '호', '비', '닿', '살', '들어가', '뿜', '찌우', '늘어나', '번지', '뺏', '닫', '돼', '이루어지', '따르', '얼', '벌', '새롭', '내', '불', '들리', '자라', '드러', '돌이키', '아이', '덮이', '크', '살펴보', '즐기', '짓', '되돌', '드시', '뿌리내리', '부르', '덜', '두', '되돌리', '둘러싸이', '어떠', '줄이', '메', '눕', '아끼', '놓이', '내려오', '웃', '떠오', '자르', '나오', '떨어지', '마치', '가리', '모이', '미치', '타', '남기', '커', '짜이', '빌', '듣', '땋', '잡', '쫒기', '내보이', '개', '끄', '끼치', '닥치', '기다리', '올려다보', '돋', '떠나', '원하', '투덜대', '두려워하', '끝', '통틀', '털', '무너지', '떨', '빻', '빨', '나아지', '데려가', '넘어가', '비싸', '잃어버리', '빠지', '올라가', '합치', '캐', '이어지', '앞서', '개이', '이끌', '모르', '줄', '믿기', '데치', '절', '띄우', '나뉘', '취하', '비하', '우리', '짜', '쌓이', '부추기', '붙이', '디', '불르', '새', '넘기', '쏟아지', '끌', '기울', '구하', '끓', '키우', '거치', '살아가', '울', '물리', '알아차리', '높이', '꼽', '깔리', '찍히', '까', '흘르', '들여다보', '솟구치', '해보', '가져가', '해내', '정하', '묻', '밭이', '끝나', '매달리', '니', '스치', '헤', '아스', '속하', '질', '날아오', '날리', '잡히', '틀리', '나타내', '벌어지', '무너뜨리', '파내', '나서', '비키', '둘러싸', '떠올리', '넓', '가리키', '다그', '띄', '올리', '맡기', '지나치', '깨어나', '붙', '배우', '익히', '이루', '지저귀', '보내오', '붇', '내리', '태어나', '히', '쓰이', '더하', '낳', '지르', '전하', '찾아가', '매이', '샬', '거들', '돌아오', '지니', '만나', '벗기', '알리', '기', '블', '받아들이', '몰', '알아채', '대리', '빼앗', '즐거우', '일', '더불', '바라', '허', '추', '사라지', '잊', '저미', '웃기', '돌', '와', '지키', '나누', '어기', '누', '모으', '늦추', '쏟', '여쭈', '들이', '생겨나', '가르', '차', '돕', '옮', '앞지르', '달려가', '처하', '나쁘', '어리', '저러', '밀', '앉히', '동이', '느껴지', '펴', '옮기', '쏠', '들이마시', '접', '빼놓', '바뀌', '묶이', '카', '팔', '쏘', '멀', '그늘지', '다르', '걸어오', '던지', '맞추', '남', '갓나', '뛰어나', '되돌아가', '벗겨지', '퍼지', '익', '몰르', '거들뜨', '이라', '델', '거두', '세', '긁', '세우', '벗어나', '헹구', '흔들', '드러나', '신나', '누리', '펼치', '뱉', '태우', '싸', '돌리', '지', '허물', '데리', '대', '바쁘', '헤어지', '달리', '깁', '불리', '뿔', '틀', '씌우', '비치', '휩쓸', '계시', '이렇', '늘리', '잊어버리', '두렵', '쳐지', '버티', '상하', '걸어가', '감', '몰아내', '쉐', '사로잡히', '섞', '뚫', '떠밀', '안기', '싫', '끌어내', '맑', '가늘', '가시', '모시', '시리', '다가오', '어둡', '들어오', '뽑', '멈추', '참', '다가가', '따라가', '괴롭히', '향하', '맡', '놓치', '이기', '밀치', '밟히', '부러지', '붙잡', '터지', '날', '달라지', '채우'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_headwords.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_R(w,h_0)=\\sum_{h\\in\\text{Seen}_R(w)}{\\text{sim}(h_0,h)\\cdot \\phi_R(w,h)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:28:50.846378Z",
     "start_time": "2020-02-14T12:28:50.840423Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_selectional_association(predicate, headword, lines, dataframe,metric):\n",
    "    '''\n",
    "    desc\n",
    "    주어진 술어와 표제어에 대해서 선택 관련도 점수를 구하는 함수\n",
    "    params\n",
    "    * predicate: 술어\n",
    "    * headword: 표제어\n",
    "    * lines: 코퍼스\n",
    "    * dataframe: 특징 벡터들을 담은 데이터프레임\n",
    "    * metric : 유사도를 구하는 함수\n",
    "    '''\n",
    "    v1 = torch.FloatTensor(dataframe.loc[headword].values)\n",
    "    seens = seen_headwords[predicate]\n",
    "    \n",
    "    total = 0\n",
    "    for seen in seens:\n",
    "        try:\n",
    "            v2 = torch.FloatTensor(dataframe.loc[seen].values)\n",
    "            total += metric(v1,v2)\n",
    "        except:\n",
    "            pass\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:55:24.091081Z",
     "start_time": "2020-02-14T12:55:24.063182Z"
    }
   },
   "outputs": [],
   "source": [
    "co = torch.load('co.pth')\n",
    "def wsd(predicate, headwords):\n",
    "    selectional_associations = []\n",
    "    for h in headwords:\n",
    "        selectional_associations +=[get_selectional_association(predicate, h , lines, co, get_cosine_similarity)] # 위에서 선언함\n",
    "    print(selectional_associations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T12:55:47.902417Z",
     "start_time": "2020-02-14T12:55:47.447612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(237.5528), tensor(219.2429), tensor(216.2626)]\n"
     ]
    }
   ],
   "source": [
    "wsd('가',['학교','사람','질문'])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
